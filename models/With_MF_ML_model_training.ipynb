{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuelOtt-code/Project-DC/blob/master/models/With_MF_ML_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dAiwrfD1zPP6",
        "outputId": "a4c1f93f-ee7b-44d2-d873-55f474f52dd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master/func.py\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_Phenol_training_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_Phenol_test_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_All_test_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_All_training_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_Phenol_training_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_Phenol_test_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_All_test_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_All_training_data.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gq5PxY19yH_",
        "outputId": "2a4eed9d-d9b3-4b6a-d6ce-39e02cd671f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-22 15:57:27--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master/func.py\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/func.py [following]\n",
            "--2025-05-22 15:57:27--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/func.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6467 (6.3K) [text/plain]\n",
            "Saving to: ‘func.py’\n",
            "\n",
            "\rfunc.py               0%[                    ]       0  --.-KB/s               \rfunc.py             100%[===================>]   6.32K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-22 15:57:27 (69.9 MB/s) - ‘func.py’ saved [6467/6467]\n",
            "\n",
            "--2025-05-22 15:57:27--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_Phenol_training_data.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_Phenol_training_data.csv [following]\n",
            "--2025-05-22 15:57:27--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_Phenol_training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 91559 (89K) [text/plain]\n",
            "Saving to: ‘Landrum_Phenol_training_data.csv’\n",
            "\n",
            "Landrum_Phenol_trai 100%[===================>]  89.41K  --.-KB/s    in 0.02s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-22 15:57:27 (3.81 MB/s) - ‘Landrum_Phenol_training_data.csv’ saved [91559/91559]\n",
            "\n",
            "--2025-05-22 15:57:27--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_Phenol_test_data.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_Phenol_test_data.csv [following]\n",
            "--2025-05-22 15:57:27--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_Phenol_test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22912 (22K) [text/plain]\n",
            "Saving to: ‘Landrum_Phenol_test_data.csv’\n",
            "\n",
            "Landrum_Phenol_test 100%[===================>]  22.38K  --.-KB/s    in 0.002s  \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-22 15:57:27 (9.04 MB/s) - ‘Landrum_Phenol_test_data.csv’ saved [22912/22912]\n",
            "\n",
            "--2025-05-22 15:57:27--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_All_test_data.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_All_test_data.csv [following]\n",
            "--2025-05-22 15:57:28--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_All_test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36568 (36K) [text/plain]\n",
            "Saving to: ‘Landrum_All_test_data.csv’\n",
            "\n",
            "Landrum_All_test_da 100%[===================>]  35.71K  --.-KB/s    in 0.01s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-22 15:57:28 (3.39 MB/s) - ‘Landrum_All_test_data.csv’ saved [36568/36568]\n",
            "\n",
            "--2025-05-22 15:57:28--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_All_training_data.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_All_training_data.csv [following]\n",
            "--2025-05-22 15:57:28--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_All_training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144784 (141K) [text/plain]\n",
            "Saving to: ‘Landrum_All_training_data.csv’\n",
            "\n",
            "Landrum_All_trainin 100%[===================>] 141.39K  --.-KB/s    in 0.03s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-22 15:57:28 (4.26 MB/s) - ‘Landrum_All_training_data.csv’ saved [144784/144784]\n",
            "\n",
            "--2025-05-22 15:57:28--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_Phenol_training_data.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_Phenol_training_data.csv [following]\n",
            "--2025-05-22 15:57:28--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_Phenol_training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99441 (97K) [text/plain]\n",
            "Saving to: ‘Minimal_Phenol_training_data.csv’\n",
            "\n",
            "Minimal_Phenol_trai 100%[===================>]  97.11K  --.-KB/s    in 0.02s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-22 15:57:28 (4.13 MB/s) - ‘Minimal_Phenol_training_data.csv’ saved [99441/99441]\n",
            "\n",
            "--2025-05-22 15:57:28--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_Phenol_test_data.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_Phenol_test_data.csv [following]\n",
            "--2025-05-22 15:57:28--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_Phenol_test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24831 (24K) [text/plain]\n",
            "Saving to: ‘Minimal_Phenol_test_data.csv’\n",
            "\n",
            "Minimal_Phenol_test 100%[===================>]  24.25K  --.-KB/s    in 0.001s  \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-22 15:57:28 (17.4 MB/s) - ‘Minimal_Phenol_test_data.csv’ saved [24831/24831]\n",
            "\n",
            "--2025-05-22 15:57:28--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_All_test_data.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_All_test_data.csv [following]\n",
            "--2025-05-22 15:57:29--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_All_test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41749 (41K) [text/plain]\n",
            "Saving to: ‘Minimal_All_test_data.csv’\n",
            "\n",
            "Minimal_All_test_da 100%[===================>]  40.77K  --.-KB/s    in 0.009s  \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-22 15:57:29 (4.34 MB/s) - ‘Minimal_All_test_data.csv’ saved [41749/41749]\n",
            "\n",
            "--2025-05-22 15:57:29--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_All_training_data.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_All_training_data.csv [following]\n",
            "--2025-05-22 15:57:29--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_All_training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163693 (160K) [text/plain]\n",
            "Saving to: ‘Minimal_All_training_data.csv’\n",
            "\n",
            "Minimal_All_trainin 100%[===================>] 159.86K  --.-KB/s    in 0.03s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-22 15:57:29 (4.56 MB/s) - ‘Minimal_All_training_data.csv’ saved [163693/163693]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import pandas as pd\n",
        " #all CSV files -> dataframe\n",
        " df_Landrum_Phenol_training_data = pd.read_csv(\"Landrum_Phenol_training_data.csv\")\n",
        " df_Landrum_Phenol_test_data = pd.read_csv(\"Landrum_Phenol_test_data.csv\")\n",
        " df_Landrum_All_training_data = pd.read_csv(\"Landrum_All_training_data.csv\")\n",
        " df_Landrum_All_test_data = pd.read_csv(\"Landrum_All_test_data.csv\")\n",
        " df_Minimal_Phenol_training_data = pd.read_csv(\"Minimal_Phenol_training_data.csv\")\n",
        " df_Minimal_Phenol_test_data = pd.read_csv(\"Minimal_Phenol_test_data.csv\")\n",
        " df_Minimal_All_training_data = pd.read_csv(\"Minimal_All_training_data.csv\")\n",
        " df_Minimal_All_test_data = pd.read_csv(\"Minimal_All_test_data.csv\")\n"
      ],
      "metadata": {
        "id": "LZuOfRtYFG_B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {\n",
        "    \"Landrum_Phenol_training\": df_Landrum_Phenol_training_data,\n",
        "    \"Landrum_All_training\": df_Landrum_All_training_data,\n",
        "    \"Landrum_Phenol_test\": df_Landrum_Phenol_test_data,\n",
        "    \"Landrum_All_test\": df_Landrum_All_test_data,\n",
        "    \"Minimal_Phenol_training\": df_Minimal_Phenol_training_data,\n",
        "    \"Minimal_All_training\": df_Minimal_All_training_data,\n",
        "    \"Minimal_Phenol_test\": df_Minimal_Phenol_test_data,\n",
        "    \"Minimal_All_test\": df_Minimal_All_test_data\n",
        "}\n",
        "\n",
        "COMPUTE = {\n",
        "    \"Landrum_Phenol_training\": False,\n",
        "    \"Landrum_All_training\": True,\n",
        "    \"Landrum_Phenol_test\": False,\n",
        "    \"Landrum_All_test\": True,\n",
        "    \"Minimal_Phenol_training\": False,\n",
        "    \"Minimal_All_training\": False,\n",
        "    \"Minimal_Phenol_test\": False,\n",
        "    \"Minimal_All_test\": False\n",
        "}"
      ],
      "metadata": {
        "id": "SAToRM6nKvFU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Features\n",
        "\n",
        "\n",
        "\n",
        "*   all from RDKit\n",
        "*  all from mordred\n",
        "*   and add all the the same dataframe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yffvZa40CPR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: provide a code that generates some features available from rdkit for the structures given in df_curated\n",
        "\n",
        "def generate_some_rdkit_features(smiles):\n",
        "    \"\"\"Generates RDKit features for a given SMILES string.\"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None  # Handle invalid SMILES\n",
        "\n",
        "    features = {}\n",
        "    # Descriptors\n",
        "    features['MolWt'] = Descriptors.MolWt(mol)\n",
        "    features['LogP'] = Descriptors.MolLogP(mol)\n",
        "    features['TPSA'] = rdMolDescriptors.CalcTPSA(mol)\n",
        "    # ... add other RDKit descriptors as needed ...\n",
        "    # 2D Descriptors\n",
        "    features['NumHAcceptors'] = Descriptors.NumHAcceptors(mol)\n",
        "    features['NumHDonors'] = Descriptors.NumHDonors(mol)\n",
        "    features['NumRotatableBonds'] = Descriptors.NumRotatableBonds(mol)\n",
        "    features['RingCount'] = Descriptors.RingCount(mol)\n",
        "\n",
        "    # Topological Descriptors\n",
        "    features['BalabanJ'] = Descriptors.BalabanJ(mol)\n",
        "    features['BertzCT'] = Descriptors.BertzCT(mol)\n",
        "    features['HallKierAlpha'] = Descriptors.HallKierAlpha(mol)\n",
        "\n",
        "\n",
        "    # ... add other relevant features ...\n",
        "\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "8JFnQl-5CoFQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: provide a code that generates all features available from rdkit for the structures given in df_curated\n",
        "!pip install rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
        "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
        "\n",
        "def calculate_rdkit_descriptors_from_mol(smiles):\n",
        "    \"\"\"Generates RDKit features for a given SMILES string.\"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None  # Handle invalid SMILES\n",
        "\n",
        "    features = Descriptors.CalcMolDescriptors(mol)\n",
        "\n",
        "    return features\n",
        "\n",
        "def generate_all_rdkit_features(df):\n",
        "    \"\"\"\n",
        "    Calculates all RDKit features for molecules in a DataFrame and adds them as columns.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame with a 'canonical_Smiles' column.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with added RDKit features as individual columns.\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply the calculate_rdkit_descriptors_from_mol function to the 'canonical_Smiles' column\n",
        "    df['features'] = df['canonical_Smiles'].apply(calculate_rdkit_descriptors_from_mol)\n",
        "\n",
        "    # Expand the features dictionary into separate columns\n",
        "    features_df = pd.DataFrame(df['features'].tolist(), index=df.index)  # Use index of original df\n",
        "\n",
        "    # Concatenate the expanded features with the original DataFrame\n",
        "    df = pd.concat([df, features_df], axis=1)\n",
        "\n",
        "    # Drop the original features column\n",
        "    df = df.drop('features', axis=1)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "m-JmlRZ1CwS9",
        "outputId": "029f602c-4bc5-4718-d495-5ec917a620d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: provide a function to calculate all 2D descriptors from mordred using the smiles from the df dataframe and append the calculated features to the same dataframe\n",
        "\n",
        "\n",
        "\n",
        "def generate_mordred_descriptors(df):\n",
        "    \"\"\"Calculates all 2D descriptors from Mordred and appends them to the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame with a 'canonical_Smiles' column.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with added Mordred descriptors.\n",
        "    \"\"\"\n",
        "\n",
        "    calc = Calculator(descriptors, ignore_3D=True) # Initialize Mordred calculator (2D only)\n",
        "\n",
        "    # Calculate descriptors for valid molecules\n",
        "    def calculate_mordred_descriptors_for_mol(smiles):\n",
        "      mol = Chem.MolFromSmiles(smiles)\n",
        "      if mol is not None:\n",
        "          return calc(mol)\n",
        "      else:\n",
        "          return None  # Return None for invalid SMILES\n",
        "\n",
        "    df['mordred_descriptors'] = df['canonical_Smiles'].apply(calculate_mordred_descriptors_for_mol)\n",
        "\n",
        "    # Expand the Mordred descriptor dictionary into separate columns\n",
        "    mordred_df = pd.DataFrame(df['mordred_descriptors'].tolist())\n",
        "\n",
        "    # Concatenate the expanded features with the original DataFrame\n",
        "    df = pd.concat([df, mordred_df], axis=1)\n",
        "\n",
        "    # Drop the original mordred_descriptors column\n",
        "    df = df.drop('mordred_descriptors', axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ULB_pC5oDGv8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mordred\n",
        "!pip install numpy==1.23.5\n",
        "!pip install rdkit\n",
        "from mordred import Calculator, descriptors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6O0A9tIHj27",
        "outputId": "ac9e0090-fd5b-430c-bde9-2ea56a5c9db2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mordred in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six==1.* in /usr/local/lib/python3.11/dist-packages (from mordred) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.* in /usr/local/lib/python3.11/dist-packages (from mordred) (1.23.5)\n",
            "Requirement already satisfied: networkx==2.* in /usr/local/lib/python3.11/dist-packages (from mordred) (2.8.8)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate mordred features\n",
        "from rdkit import Chem\n",
        "mordred_descriptors = {}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if COMPUTE.get(name, False):\n",
        "        print(f\"✅ Computing: {name}\")\n",
        "        mordred_descriptors[name] = generate_mordred_descriptors(df)\n",
        "        mordred_descriptors[name].to_csv(f\"{name}_mordred.csv\", index=False)\n",
        "    else:\n",
        "        print(f\"⏭️ Skipping: {name}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "56ApAgPcIYA3",
        "outputId": "642e939a-6a12-474a-e8a0-f2258d57d7a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏭️ Skipping: Landrum_Phenol_training\n",
            "✅ Computing: Landrum_All_training\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2884fadbf209>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mCOMPUTE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Computing: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmordred_descriptors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_mordred_descriptors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmordred_descriptors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name}_mordred.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-407ae9089e7b>\u001b[0m in \u001b[0;36mgenerate_mordred_descriptors\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Return None for invalid SMILES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mordred_descriptors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'canonical_Smiles'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_mordred_descriptors_for_mol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Expand the Mordred descriptor dictionary into separate columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-407ae9089e7b>\u001b[0m in \u001b[0;36mcalculate_mordred_descriptors_for_mol\u001b[0;34m(smiles)\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolFromSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Return None for invalid SMILES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mordred/_base/calculator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, mol, id)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miterator\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m--> 288\u001b[0;31m         return self._wrap_result(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_calculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mordred/_base/calculator.py\u001b[0m in \u001b[0;36m_wrap_result\u001b[0;34m(self, mol, r)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_descriptors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_serial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipynb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mordred/_base/result.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mol, r, d)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_descriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_to_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mordred/_base/calculator.py\u001b[0m in \u001b[0;36m_calculate\u001b[0;34m(self, cxt)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescriptors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mordred/_base/calculator.py\u001b[0m in \u001b[0;36m_calculate_one\u001b[0;34m(self, cxt, desc, reset)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mordred/_base/calculator.py\u001b[0m in \u001b[0;36m_calculate_one\u001b[0;34m(self, cxt, desc, reset)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_rtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mordred/PathCount.py\u001b[0m in \u001b[0;36mcalculate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_bonds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFindAllPathsOfLengthN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0maids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages later so they do not collide with mordred\n",
        "# Install RDKit\n",
        "!pip install rdkit\n",
        "\n",
        "# Library imports\n",
        "from pathlib import Path\n",
        "from warnings import filterwarnings\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# RDKit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn import svm, metrics, clone\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import (\n",
        "    auc, accuracy_score, recall_score, roc_curve, roc_auc_score, RocCurveDisplay\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import (\n",
        "    VarianceThreshold, SelectKBest, f_classif, mutual_info_classif, chi2\n",
        ")\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import cohen_kappa_score, confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "metadata": {
        "id": "-HV5tE0Msg6t",
        "outputId": "c6b43735-e868-4a03-dcb5-6cf4150b2987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "thoXTc4lJkqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selection\n",
        "\n",
        "* Remove semi-constant features (>= 80% of column values the same after\n",
        "* Lu, A. (2022) https://doi.org/10.1038/s41598-022-11925-y)\n",
        "Remove highly correlating columns. Threshold to be discussed, maybe 0.75?"
      ],
      "metadata": {
        "id": "y6av1wOBDe8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#alternative feature selection\n",
        "\n",
        "def drop_non_numeric_columns(df):\n",
        "    \"\"\"Drops columns from a DataFrame that do not contain numeric values.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with non-numeric columns removed.\n",
        "    \"\"\"\n",
        "    numeric_df = df.select_dtypes(include=np.number)\n",
        "    return numeric_df"
      ],
      "metadata": {
        "id": "HafvI2F1I20r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#old feature selectionfunctions (probably delete)\n",
        "def remove_semi_constant_features(df, semi_constant_threshold):\n",
        "    \"\"\"Removes columns from a Pandas DataFrame where 80% or more of the values are the same.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with the single-value columns removed.\n",
        "    \"\"\"\n",
        "\n",
        "    cols_to_drop = []\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            # Calculate the percentage of the most frequent value\n",
        "            counts = df[col].value_counts(normalize=True)\n",
        "            if counts.iloc[0] >= semi_constant_threshold:\n",
        "                cols_to_drop.append(col)\n",
        "        except (TypeError, IndexError):\n",
        "            pass  # Handle cases where value_counts fails (e.g., mixed data types)\n",
        "\n",
        "    return df.drop(columns=cols_to_drop)\n",
        "\n",
        "def drop_non_numeric_columns(df):\n",
        "    \"\"\"Drops columns from a DataFrame that do not contain numeric values.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with non-numeric columns removed.\n",
        "    \"\"\"\n",
        "    numeric_df = df.select_dtypes(include=np.number)\n",
        "    return numeric_df\n",
        "def drop_high_correlation_columns(df, threshold=0.75):\n",
        "    \"\"\"Drops columns from a DataFrame that have a correlation above a specified threshold.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "        threshold: The correlation threshold above which columns are dropped.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with highly correlated columns removed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    corr_matrix = df.corr().abs()\n",
        "\n",
        "    # Select upper triangle of correlation matrix\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    # Find index of feature columns with correlation greater than 0.75\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "\n",
        "    # Drop features\n",
        "    return df.drop(columns=to_drop)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mjTpMu0nVGiq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Landrum = True\n",
        "Phenol = False\n",
        "\n",
        "if Landrum:\n",
        "    if Phenol:\n",
        "        df_training = pd.read_csv(\"Landrum_Phenol_training_mordred.csv\")\n",
        "        df_test = pd.read_csv(\"Landrum_Phenol_test_mordred.csv\")\n",
        "    else:\n",
        "        df_training = pd.read_csv(\"Landrum_All_training_mordred.csv\")\n",
        "        df_test = pd.read_csv(\"Landrum_All_test_mordred.csv\")\n",
        "else:\n",
        "    if Phenol:\n",
        "        df_training = pd.read_csv(\"Minimal_Phenol_training_mordred.csv\")\n",
        "        df_test = pd.read_csv(\"Minimal_Phenol_test_mordred.csv\")\n",
        "    else:\n",
        "        df_training = pd.read_csv(\"Minimal_All_training_mordred.csv\")\n",
        "        df_test = pd.read_csv(\"Minimal_All_test_mordred.csv\")\n",
        "\n",
        "# === Normalize column names just once ===\n",
        "df_training.columns = df_training.columns.str.strip().str.lower()\n",
        "df_test.columns = df_test.columns.str.strip().str.lower()\n",
        "\n",
        "# Configuration switches\n",
        "USE_MORDRED = True       # True for Mordred, False for Morgan\n",
        "\n",
        "if USE_MORDRED:\n",
        "    # Drop non-numeric columns and NaNs\n",
        "    df_training = drop_non_numeric_columns(df_training)\n",
        "    df_training = df_training.dropna(axis=1)\n",
        "    df_test = drop_non_numeric_columns(df_test)\n",
        "    df_test = df_test.dropna(axis=1)\n",
        "\n",
        "    # Prepare features and labels\n",
        "    X_train = df_training.drop(columns=[\"active\", \"is_phenol\"], errors='ignore')\n",
        "    y_train = df_training[\"active\"]\n",
        "    X_test = df_test.drop(columns=[\"active\", \"is_phenol\"], errors='ignore')\n",
        "    y_test = df_test[\"active\"]\n",
        "\n",
        "\n",
        "else:\n",
        "    # Process Morgan fingerprints from SMILES\n",
        "    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
        "\n",
        "    def smiles_to_fp(smiles):\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return np.nan\n",
        "        return mfpgen.GetFingerprintAsNumPy(mol)\n",
        "\n",
        "    df_training[\"fp\"] = df_training[\"canonical_smiles\"].apply(smiles_to_fp)\n",
        "    df_training = df_training[df_training[\"fp\"].notna()].copy()\n",
        "    df_test[\"fp\"] = df_test[\"canonical_smiles\"].apply(smiles_to_fp)\n",
        "    df_test = df_test[df_test[\"fp\"].notna()].copy()\n",
        "\n",
        "\n",
        "    X_training = pd.DataFrame(df_training[\"fp\"].to_list())\n",
        "    y_training = df_training[\"active\"]\n",
        "    X_test = pd.DataFrame(df_test[\"fp\"].to_list())\n",
        "    y_test = df_test[\"active\"]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JcjiMHWoUFcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7dfed08-2eda-4ee1-b7c5-ecf846ff8a27"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-70533c7df315>:9: DtypeWarning: Columns (7,8,9,10,11,12,13,14,15,16,17,18,56,57,58,59,60,61,62,63,64,143,144,152,153,155,156,157,158,159,160,161,170,171,179,180,188,189,197,198,206,207,215,216,224,225,233,234,236,237,238,239,240,241,242,243,244,263,264,265,266,267,268,269,270,271,344,345,346,347,348,349,350,359,360,368,369,371,372,373,374,375,376,377,386,387,395,396,404,405,413,414,422,423,431,432,440,441,449,450,452,453,454,455,456,457,465,466,473,474,476,477,478,479,480,481,489,490,497,498,505,506,513,514,521,522,529,530,537,538,545,546,548,549,550,551,552,553,561,562,569,570,572,573,574,575,576,577,585,586,593,594,601,602,609,610,617,618,625,626,633,634,641,642,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,783,784,820,828,834,835,836,844,850,851,882,883,884,885,886,887,888,889,890,891,892,893,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1366,1367,1369,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1585,1614) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_training = pd.read_csv(\"Landrum_All_training_mordred.csv\")\n",
            "<ipython-input-17-70533c7df315>:10: DtypeWarning: Columns (236,237,238,239,240,241,242,243,244,344,345,346,347,348,349,350,452,453,454,455,456,457,548,549,550,551,552,553,783,784,833,849,1585) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_test = pd.read_csv(\"Landrum_All_test_mordred.csv\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_np = X_train.to_numpy()\n",
        "y_train_np = y_train.to_numpy()\n",
        "X_test_np = X_test.to_numpy()\n",
        "y_test_np = y_test.to_numpy()\n",
        "\n",
        "print(X_train_np[:5])"
      ],
      "metadata": {
        "id": "sxkAf8bG5ASz",
        "outputId": "1d68a221-6637-4a7f-8fde-5db6628f3431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 13.23443095  11.54854293   1.         ...  82.          88.\n",
            "    4.11111111]\n",
            " [ 31.0649762   21.62099313   0.         ... 220.         272.\n",
            "    8.        ]\n",
            " [ 17.2868016   14.56386614   0.         ... 114.         131.\n",
            "    5.25      ]\n",
            " [ 23.84538077  19.03224991   0.         ... 158.         183.\n",
            "    6.83333333]\n",
            " [ 16.33240565  13.45331614   0.         ... 106.         120.\n",
            "    5.19444444]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Linktext](https://)#Model training"
      ],
      "metadata": {
        "id": "b-hx7MTkZicV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Preprocessing and Models\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def get_model_pipeline(model_name):\n",
        "    if model_name == 'RandomForest':\n",
        "        model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "    elif model_name == 'SVM':\n",
        "        model = SVC(class_weight='balanced', probability=True, random_state=42)\n",
        "    elif model_name == 'LogisticRegression':\n",
        "        model = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
        "    elif model_name == 'NeuralNetwork':\n",
        "        model = MLPClassifier(max_iter=1000, random_state=42)\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} is not supported.\")\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', MinMaxScaler()), #\n",
        "        (\"var_thresh\", VarianceThreshold(threshold=0.01)),#light filtering of low variance features (more often seen than the 80% constant value method but should do approx the same)\n",
        "        (\"anova\", SelectKBest(score_func=mutual_info_classif)),  # Supervised Filter-Based Selection, k can be tuned (chi2 for morgan, f_classif for mordred, mutual_info_classif can handle both but takes long)\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "Kq4cDyFjZHxL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grids = {\n",
        "    'RandomForest': {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [None, 10, 20],\n",
        "        'anova__k': [50, 100, 200],\n",
        "        'classifier__max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'SVM': {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'anova__k': [50, 100, 200],\n",
        "        'classifier__kernel': ['linear', 'rbf']\n",
        "    },\n",
        "    'LogisticRegression': {\n",
        "    'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'classifier__C': [0.1, 0.01, 0.001],\n",
        "    'classifier__solver': ['saga'],\n",
        "    'anova__k': [50, 100, 200],\n",
        "    'classifier__l1_ratio': [0.25, 0.5, 0.75]\n",
        "    },\n",
        "    'NeuralNetwork': {\n",
        "        'classifier__hidden_layer_sizes': [(50,), (100,100), (50, 50)],\n",
        "        'classifier__alpha': [0.0001, 0.001],\n",
        "        'anova__k': [50, 100, 200]\n",
        "\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "h-PUUt6FUxqr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Tuning with Cross-Validation\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "def perform_grid_search(pipeline, param_grid, X_train, y_train):\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_grid=param_grid,\n",
        "        cv=kf,\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search\n"
      ],
      "metadata": {
        "id": "Y0A96ZSWVDCx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, precision_score, f1_score, roc_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.base import clone\n",
        "\n",
        "#everything called test has nothing to do with test set (is validation but called test in scikit learn)\n",
        "def evaluate_model_with_cv(model, X, y, random_state):\n",
        "    \"\"\"\n",
        "    Evaluate a model using 10-fold cross-validation and return performance metrics.\n",
        "\n",
        "    Parameters:\n",
        "        model: The model to evaluate\n",
        "        X: Feature matrix\n",
        "        y: Target vector\n",
        "        random_state: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with performance metrics and arrays of per-fold metrics\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    # Initialize KFold\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
        "\n",
        "\n",
        "    # Lists to store metrics\n",
        "    metrics = {\n",
        "        'accuracies': [],\n",
        "        'precisions': [],\n",
        "        'f1_scores': [],\n",
        "        'auc_scores': [],\n",
        "        'kappa_scores': []\n",
        "    }\n",
        "\n",
        "    # Create figure for ROC curves\n",
        "    plt.figure()\n",
        "\n",
        "    # Iterate over folds\n",
        "    for k, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "        # Split data\n",
        "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "        y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Predict\n",
        "        y_pred_fold = model.predict(X_test_fold)\n",
        "        y_prob_fold = model.predict_proba(X_test_fold)[:, 1]\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics['accuracies'].append(accuracy_score(y_test_fold, y_pred_fold))\n",
        "        metrics['precisions'].append(precision_score(y_test_fold, y_pred_fold))\n",
        "        metrics['f1_scores'].append(f1_score(y_test_fold, y_pred_fold))\n",
        "        metrics['auc_scores'].append(roc_auc_score(y_test_fold, y_prob_fold))\n",
        "        metrics['kappa_scores'].append(cohen_kappa_score(y_test_fold, y_pred_fold))\n",
        "\n",
        "        # ROC curve\n",
        "        RocCurveDisplay.from_predictions(y_test_fold, y_prob_fold, name=f\"Fold {k+1}\", ax=plt.gca())\n",
        "\n",
        "\n",
        "    # Print metrics\n",
        "    metric_display_names = {\n",
        "    'accuracies': 'accuracy',\n",
        "    'precisions':'precision',\n",
        "    'f1_scores': 'F1 score',\n",
        "    'auc_scores': 'AUC score',\n",
        "    'kappa_scores': 'Cohens kappa'\n",
        "    }\n",
        "\n",
        "    for metric_name, values in metrics.items():\n",
        "      display_name = metric_display_names[metric_name]\n",
        "      print(f\"Average {display_name}: {np.mean(values):.4f} \\t and std err: {stats.sem(values):.4f}\")\n",
        "    print(f\"Time taken : {time.time() - t0:.2f}s\\n\")\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "X6rxmxMHNI8Q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through each model, perform grid search, and evaluate\n",
        "all_metrics = []\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import sem\n",
        "for model_name in ['NeuralNetwork']: #add models to train: 'NeuralNetwork','RandomForest', 'SVM', 'LogisticRegression',\n",
        "    print(f\"\\nTraining and evaluating {model_name}...\")\n",
        "    t0 = time.time()\n",
        "    pipeline = get_model_pipeline(model_name)\n",
        "    param_grid = param_grids[model_name]\n",
        "    grid_search = perform_grid_search(pipeline, param_grid, X_train, y_train)\n",
        "    model = grid_search.best_estimator_\n",
        "    print(f\"Time taken : {time.time() - t0:.2f}s\\n\")\n",
        "    print(f\"Hyperparameters: {grid_search.best_params_}\\n\")\n",
        "    metrics_model = evaluate_model_with_cv(model, X_train, y_train, random_state=22)\n",
        "    df_metrics = pd.DataFrame(metrics_model)\n",
        "\n",
        "    df_metrics['Model'] = model_name\n",
        "\n",
        "    all_metrics.append(df_metrics) #get list with all metrics for each model\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "J8ew0D9LjsYz",
        "outputId": "3d6d16c7-fbd7-49f3-92b9-850226b2c063"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and evaluating NeuralNetwork...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:409: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.array(param_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken : 1468.26s\n",
            "\n",
            "Hyperparameters: {'anova__k': 200, 'classifier__alpha': 0.001, 'classifier__hidden_layer_sizes': (50, 50), 'classifier__learning_rate': 'adaptive'}\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\\n       ...\\n       2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2610, 2612],\\n      dtype='int64', length=2351)] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e31c38eea8eb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time taken : {time.time() - t0:.2f}s\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hyperparameters: {grid_search.best_params_}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mevaluate_model_with_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-2ff54755e205>\u001b[0m in \u001b[0;36mevaluate_model_with_cv\u001b[0;34m(model, X, y, random_state)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Split data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mX_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0my_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\\n       ...\\n       2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2610, 2612],\\n      dtype='int64', length=2351)] are in the [columns]\""
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\large Evaluation"
      ],
      "metadata": {
        "id": "P-PNwkmMo3BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_statistical_classifier(probability, X, y, random_state):\n",
        "    outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
        "    metrics = {\n",
        "        'accuracies': [],\n",
        "        'precisions': [],\n",
        "        'recalls': [],\n",
        "        'f1_scores': [],\n",
        "        'auc_scores': [],\n",
        "        'kappas': []\n",
        "    }\n",
        "\n",
        "    def get_activity(probability, length_of_y):\n",
        "        rng = np.random.default_rng(seed=random_state)\n",
        "        y_predictions = []\n",
        "        for _ in range(length_of_y):\n",
        "            random_num = rng.random()\n",
        "            if random_num <= probability:\n",
        "                activity = 1\n",
        "            else:\n",
        "                activity = 0\n",
        "            y_predictions.append(activity)\n",
        "        return np.array(y_predictions)\n",
        "\n",
        "    y_predictions = get_activity(probability, len(y)) #predicting the activity based on chance\n",
        "\n",
        "    for k, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        y_pred = y_predictions[test_idx]  # get predictions for current test fold\n",
        "        y_proba = np.full_like(y_pred, fill_value=y_pred.mean(), dtype=float)  # probabilities as mean activity in this fold and has the same shape as predictions\n",
        "\n",
        "        # Calculate metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_test, y_proba)\n",
        "        except ValueError:\n",
        "            auc = np.nan  # in case y_test has one class only\n",
        "        kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "        metrics['accuracies'].append(acc)\n",
        "        metrics['precisions'].append(prec)\n",
        "        metrics['recalls'].append(rec)\n",
        "        metrics['f1_scores'].append(f1)\n",
        "        metrics['auc_scores'].append(auc)\n",
        "        metrics['kappas'].append(kappa)\n",
        "\n",
        "        RocCurveDisplay.from_predictions(y_test, y_proba, name=f\"Fold {k+1}\", ax=plt.gca())\n",
        "\n",
        "    print(\"\\nEvaluation Metrics (10-fold Stratified CV):\")\n",
        "    for key, values in metrics.items():\n",
        "        mean = np.nanmean(values)\n",
        "        stderr = stats.sem([v for v in values if not np.isnan(v)])\n",
        "        print(f\"{key.capitalize():<12} Mean = {mean:.4f}, StdErr = {stderr:.4f}\")\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "5cdxVmE9ovpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the metrics for the dummy classifier\n",
        "metrics_statistical = evaluate_statistical_classifier(probability=np.mean(y), X=X_train, y=y_train, random_state=22)\n",
        "metrics_statistical_df = pd.DataFrame(metrics_statistical)\n",
        "metrics_statistical_df[\"Model\"] = \"Statistical\"\n",
        "all_metrics.append(metrics_statistical_df)\n",
        "metrics_statistical_df\n",
        "print(all_metrics)"
      ],
      "metadata": {
        "id": "_Fjm3Arcgvpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the barplot with all the metrics\n",
        "import seaborn as sns\n",
        "\n",
        "metrics_melted = metrics_all_models_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
        "\n",
        "sns.barplot(data=metrics_melted, x='Metric', y='Score', hue='Model', ci='sd')\n",
        "plt.title(\"Comparison of Evaluation Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NJPObLayp2YW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}