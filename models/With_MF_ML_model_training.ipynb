{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuelOtt-code/Project-DC/blob/master/models/With_MF_ML_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "Ov0xHwD5Zhqw",
        "outputId": "43c5b6fb-7a34-4e45-eea0-93c0aa041f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.3.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n",
            "Downloading rdkit-2025.3.2-cp311-cp311-manylinux_2_28_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.3.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-13d532c41df8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "#import packages\n",
        "# Install RDKit\n",
        "!pip install rdkit\n",
        "\n",
        "# Library imports\n",
        "from pathlib import Path\n",
        "from warnings import filterwarnings\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# RDKit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn import svm, metrics, clone\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import (\n",
        "    auc, accuracy_score, recall_score, roc_curve, roc_auc_score, RocCurveDisplay\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import (\n",
        "    VarianceThreshold, SelectKBest, f_classif, mutual_info_classif, chi2\n",
        ")\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import cohen_kappa_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dAiwrfD1zPP6",
        "outputId": "6e3f6017-f815-4cce-cc7d-34ec8a820a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master/func.py\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_Phenol_training_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_Phenol_test_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_All_test_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_All_training_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_Phenol_training_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_Phenol_test_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_All_test_data.csv\n",
        "!wget --timestamping https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_All_training_data.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gq5PxY19yH_",
        "outputId": "db137c02-3a49-4ba5-b1c5-e549f5c24cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-19 23:09:56--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master/func.py\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/func.py [following]\n",
            "--2025-05-19 23:09:56--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/func.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6467 (6.3K) [text/plain]\n",
            "Saving to: ‘func.py’\n",
            "\n",
            "func.py             100%[===================>]   6.32K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-19 23:09:56 (53.6 MB/s) - ‘func.py’ saved [6467/6467]\n",
            "\n",
            "--2025-05-19 23:09:56--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_Phenol_training_data.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_Phenol_training_data.csv [following]\n",
            "--2025-05-19 23:09:56--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_Phenol_training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 91559 (89K) [text/plain]\n",
            "Saving to: ‘Landrum_Phenol_training_data.csv’\n",
            "\n",
            "Landrum_Phenol_trai 100%[===================>]  89.41K  --.-KB/s    in 0.02s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-19 23:09:57 (3.61 MB/s) - ‘Landrum_Phenol_training_data.csv’ saved [91559/91559]\n",
            "\n",
            "--2025-05-19 23:09:57--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_Phenol_test_data.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_Phenol_test_data.csv [following]\n",
            "--2025-05-19 23:09:57--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_Phenol_test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22912 (22K) [text/plain]\n",
            "Saving to: ‘Landrum_Phenol_test_data.csv’\n",
            "\n",
            "Landrum_Phenol_test 100%[===================>]  22.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-19 23:09:57 (16.1 MB/s) - ‘Landrum_Phenol_test_data.csv’ saved [22912/22912]\n",
            "\n",
            "--2025-05-19 23:09:57--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_All_test_data.csv\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_All_test_data.csv [following]\n",
            "--2025-05-19 23:09:57--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_All_test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36568 (36K) [text/plain]\n",
            "Saving to: ‘Landrum_All_test_data.csv’\n",
            "\n",
            "Landrum_All_test_da 100%[===================>]  35.71K  --.-KB/s    in 0.01s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-19 23:09:58 (3.38 MB/s) - ‘Landrum_All_test_data.csv’ saved [36568/36568]\n",
            "\n",
            "--2025-05-19 23:09:58--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Landrum_All_training_data.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_All_training_data.csv [following]\n",
            "--2025-05-19 23:09:58--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Landrum_All_training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144784 (141K) [text/plain]\n",
            "Saving to: ‘Landrum_All_training_data.csv’\n",
            "\n",
            "Landrum_All_trainin 100%[===================>] 141.39K  --.-KB/s    in 0.03s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-19 23:09:58 (4.45 MB/s) - ‘Landrum_All_training_data.csv’ saved [144784/144784]\n",
            "\n",
            "--2025-05-19 23:09:58--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_Phenol_training_data.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_Phenol_training_data.csv [following]\n",
            "--2025-05-19 23:09:58--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_Phenol_training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99441 (97K) [text/plain]\n",
            "Saving to: ‘Minimal_Phenol_training_data.csv’\n",
            "\n",
            "Minimal_Phenol_trai 100%[===================>]  97.11K  --.-KB/s    in 0.02s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-19 23:09:59 (3.93 MB/s) - ‘Minimal_Phenol_training_data.csv’ saved [99441/99441]\n",
            "\n",
            "--2025-05-19 23:09:59--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_Phenol_test_data.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_Phenol_test_data.csv [following]\n",
            "--2025-05-19 23:09:59--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_Phenol_test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24831 (24K) [text/plain]\n",
            "Saving to: ‘Minimal_Phenol_test_data.csv’\n",
            "\n",
            "Minimal_Phenol_test 100%[===================>]  24.25K  --.-KB/s    in 0.002s  \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-19 23:09:59 (10.8 MB/s) - ‘Minimal_Phenol_test_data.csv’ saved [24831/24831]\n",
            "\n",
            "--2025-05-19 23:09:59--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_All_test_data.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_All_test_data.csv [following]\n",
            "--2025-05-19 23:09:59--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_All_test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24831 (24K) [text/plain]\n",
            "Saving to: ‘Minimal_All_test_data.csv’\n",
            "\n",
            "Minimal_All_test_da 100%[===================>]  24.25K  --.-KB/s    in 0.002s  \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-19 23:09:59 (12.1 MB/s) - ‘Minimal_All_test_data.csv’ saved [24831/24831]\n",
            "\n",
            "--2025-05-19 23:09:59--  https://github.com/ManuelOtt-code/Project-DC/raw/refs/heads/master//data_extraction%2Bcuration/Minimal_All_training_data.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_All_training_data.csv [following]\n",
            "--2025-05-19 23:10:00--  https://raw.githubusercontent.com/ManuelOtt-code/Project-DC/refs/heads/master/data_extraction%2Bcuration/Minimal_All_training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99441 (97K) [text/plain]\n",
            "Saving to: ‘Minimal_All_training_data.csv’\n",
            "\n",
            "Minimal_All_trainin 100%[===================>]  97.11K  --.-KB/s    in 0.02s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2025-05-19 23:10:00 (3.95 MB/s) - ‘Minimal_All_training_data.csv’ saved [99441/99441]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " #all CSV files -> dataframe\n",
        " df_Landrum_Phenol_training_data = pd.read_csv(\"Landrum_Phenol_training_data.csv\")\n",
        " df_Landrum_Phenol_test_data = pd.read_csv(\"Landrum_Phenol_test_data.csv\")\n",
        " df_Landrum_All_training_data = pd.read_csv(\"Landrum_All_training_data.csv\")\n",
        " df_Landrum_All_test_data = pd.read_csv(\"Landrum_All_test_data.csv\")\n",
        " df_Minimal_Phenol_training_data = pd.read_csv(\"Minimal_Phenol_training_data.csv\")\n",
        " df_Minimal_Phenol_test_data = pd.read_csv(\"Minimal_Phenol_test_data.csv\")\n",
        " df_Minimal_All_training_data = pd.read_csv(\"Minimal_All_training_data.csv\")\n",
        " df_Minimal_All_test_data = pd.read_csv(\"Minimal_All_test_data.csv\")\n"
      ],
      "metadata": {
        "id": "LZuOfRtYFG_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {\n",
        "    \"Landrum_Phenol_training\": df_Landrum_Phenol_training_data,\n",
        "    \"Landrum_All_training\": df_Landrum_All_training_data,\n",
        "    \"Landrum_Phenol_test\": df_Landrum_Phenol_test_data,\n",
        "    \"Landrum_All_test\": df_Landrum_All_test_data,\n",
        "    \"Minimal_Phenol_training\": df_Minimal_Phenol_training_data,\n",
        "    \"Minimal_All_training\": df_Minimal_All_training_data,\n",
        "    \"Minimal_Phenol_test\": df_Minimal_Phenol_test_data,\n",
        "    \"Minimal_All_test\": df_Minimal_All_test_data\n",
        "}\n",
        "\n",
        "COMPUTE = {\n",
        "    \"Landrum_Phenol_training\": False,\n",
        "    \"Landrum_All_training\": False,\n",
        "    \"Landrum_Phenol_test\": False,\n",
        "    \"Landrum_All_test\": False,\n",
        "    \"Minimal_Phenol_training\": False,\n",
        "    \"Minimal_All_training\": True,\n",
        "    \"Minimal_Phenol_test\": False,\n",
        "    \"Minimal_All_test\": True\n",
        "}"
      ],
      "metadata": {
        "id": "SAToRM6nKvFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Features\n",
        "\n",
        "\n",
        "\n",
        "*   all from RDKit\n",
        "*  all from mordred\n",
        "*   and add all the the same dataframe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yffvZa40CPR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: provide a code that generates some features available from rdkit for the structures given in df_curated\n",
        "\n",
        "def generate_some_rdkit_features(smiles):\n",
        "    \"\"\"Generates RDKit features for a given SMILES string.\"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None  # Handle invalid SMILES\n",
        "\n",
        "    features = {}\n",
        "    # Descriptors\n",
        "    features['MolWt'] = Descriptors.MolWt(mol)\n",
        "    features['LogP'] = Descriptors.MolLogP(mol)\n",
        "    features['TPSA'] = rdMolDescriptors.CalcTPSA(mol)\n",
        "    # ... add other RDKit descriptors as needed ...\n",
        "    # 2D Descriptors\n",
        "    features['NumHAcceptors'] = Descriptors.NumHAcceptors(mol)\n",
        "    features['NumHDonors'] = Descriptors.NumHDonors(mol)\n",
        "    features['NumRotatableBonds'] = Descriptors.NumRotatableBonds(mol)\n",
        "    features['RingCount'] = Descriptors.RingCount(mol)\n",
        "\n",
        "    # Topological Descriptors\n",
        "    features['BalabanJ'] = Descriptors.BalabanJ(mol)\n",
        "    features['BertzCT'] = Descriptors.BertzCT(mol)\n",
        "    features['HallKierAlpha'] = Descriptors.HallKierAlpha(mol)\n",
        "\n",
        "\n",
        "    # ... add other relevant features ...\n",
        "\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "8JFnQl-5CoFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: provide a code that generates all features available from rdkit for the structures given in df_curated\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
        "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
        "\n",
        "def calculate_rdkit_descriptors_from_mol(smiles):\n",
        "    \"\"\"Generates RDKit features for a given SMILES string.\"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None  # Handle invalid SMILES\n",
        "\n",
        "    features = Descriptors.CalcMolDescriptors(mol)\n",
        "\n",
        "    return features\n",
        "\n",
        "def generate_all_rdkit_features(df):\n",
        "    \"\"\"\n",
        "    Calculates all RDKit features for molecules in a DataFrame and adds them as columns.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame with a 'canonical_Smiles' column.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with added RDKit features as individual columns.\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply the calculate_rdkit_descriptors_from_mol function to the 'canonical_Smiles' column\n",
        "    df['features'] = df['canonical_Smiles'].apply(calculate_rdkit_descriptors_from_mol)\n",
        "\n",
        "    # Expand the features dictionary into separate columns\n",
        "    features_df = pd.DataFrame(df['features'].tolist(), index=df.index)  # Use index of original df\n",
        "\n",
        "    # Concatenate the expanded features with the original DataFrame\n",
        "    df = pd.concat([df, features_df], axis=1)\n",
        "\n",
        "    # Drop the original features column\n",
        "    df = df.drop('features', axis=1)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "m-JmlRZ1CwS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: provide a function to calculate all 2D descriptors from mordred using the smiles from the df dataframe and append the calculated features to the same dataframe\n",
        "\n",
        "\n",
        "\n",
        "def generate_mordred_descriptors(df):\n",
        "    \"\"\"Calculates all 2D descriptors from Mordred and appends them to the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame with a 'canonical_Smiles' column.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with added Mordred descriptors.\n",
        "    \"\"\"\n",
        "\n",
        "    calc = Calculator(descriptors, ignore_3D=True) # Initialize Mordred calculator (2D only)\n",
        "\n",
        "    # Calculate descriptors for valid molecules\n",
        "    def calculate_mordred_descriptors_for_mol(smiles):\n",
        "      mol = Chem.MolFromSmiles(smiles)\n",
        "      if mol is not None:\n",
        "          return calc(mol)\n",
        "      else:\n",
        "          return None  # Return None for invalid SMILES\n",
        "\n",
        "    df['mordred_descriptors'] = df['canonical_Smiles'].apply(calculate_mordred_descriptors_for_mol)\n",
        "\n",
        "    # Expand the Mordred descriptor dictionary into separate columns\n",
        "    mordred_df = pd.DataFrame(df['mordred_descriptors'].tolist())\n",
        "\n",
        "    # Concatenate the expanded features with the original DataFrame\n",
        "    df = pd.concat([df, mordred_df], axis=1)\n",
        "\n",
        "    # Drop the original mordred_descriptors column\n",
        "    df = df.drop('mordred_descriptors', axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ULB_pC5oDGv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate rdkit features\n",
        "\n",
        "rdkit_features = {}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if COMPUTE.get(name, False):\n",
        "        print(f\"✅ Computing: {name}\")\n",
        "        rdkit_features[name] = generate_all_rdkit_features(df)\n",
        "        rdkit_features[name].to_csv(f\"{name}_rdkit.csv\", index=False)\n",
        "    else:\n",
        "        print(f\"⏭️ Skipping: {name}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSqT3OHEDupg",
        "outputId": "e6b43521-de17-48fa-e1dc-7526377144a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏭️ Skipping: Landrum_Phenol_training\n",
            "⏭️ Skipping: Landrum_All_training\n",
            "⏭️ Skipping: Landrum_Phenol_test\n",
            "⏭️ Skipping: Landrum_All_test\n",
            "⏭️ Skipping: Minimal_Phenol_training\n",
            "✅ Computing: Minimal_All_training\n",
            "⏭️ Skipping: Minimal_Phenol_test\n",
            "✅ Computing: Minimal_All_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mordred\n",
        "!pip install numpy==1.23.5\n",
        "from mordred import Calculator, descriptors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6O0A9tIHj27",
        "outputId": "1c65fd43-dbe9-45b3-fd55-474a70bebae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mordred in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six==1.* in /usr/local/lib/python3.11/dist-packages (from mordred) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.* in /usr/local/lib/python3.11/dist-packages (from mordred) (1.23.5)\n",
            "Requirement already satisfied: networkx==2.* in /usr/local/lib/python3.11/dist-packages (from mordred) (2.8.8)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate mordred features\n",
        "mordred_descriptors = {}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if COMPUTE.get(name, False):\n",
        "        print(f\"✅ Computing: {name}\")\n",
        "        mordred_descriptors[name] = generate_mordred_descriptors(df)\n",
        "        mordred_descriptors[name].to_csv(f\"{name}_mordred.csv\", index=False)\n",
        "    else:\n",
        "        print(f\"⏭️ Skipping: {name}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ApAgPcIYA3",
        "outputId": "92b1ac0b-ffe7-4a41-b686-1a573544b70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏭️ Skipping: Landrum_Phenol_training\n",
            "⏭️ Skipping: Landrum_All_training\n",
            "⏭️ Skipping: Landrum_Phenol_test\n",
            "⏭️ Skipping: Landrum_All_test\n",
            "⏭️ Skipping: Minimal_Phenol_training\n",
            "✅ Computing: Minimal_All_training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏭️ Skipping: Minimal_Phenol_test\n",
            "✅ Computing: Minimal_All_test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thoXTc4lJkqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selection\n",
        "\n",
        "* Remove semi-constant features (>= 80% of column values the same after\n",
        "* Lu, A. (2022) https://doi.org/10.1038/s41598-022-11925-y)\n",
        "Remove highly correlating columns. Threshold to be discussed, maybe 0.75?"
      ],
      "metadata": {
        "id": "y6av1wOBDe8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#alternative feature selection\n",
        "\n",
        "def drop_non_numeric_columns(df):\n",
        "    \"\"\"Drops columns from a DataFrame that do not contain numeric values.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with non-numeric columns removed.\n",
        "    \"\"\"\n",
        "    numeric_df = df.select_dtypes(include=np.number)\n",
        "    return numeric_df"
      ],
      "metadata": {
        "id": "HafvI2F1I20r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#old feature selectionfunctions (probably delete)\n",
        "def remove_semi_constant_features(df, semi_constant_threshold):\n",
        "    \"\"\"Removes columns from a Pandas DataFrame where 80% or more of the values are the same.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with the single-value columns removed.\n",
        "    \"\"\"\n",
        "\n",
        "    cols_to_drop = []\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            # Calculate the percentage of the most frequent value\n",
        "            counts = df[col].value_counts(normalize=True)\n",
        "            if counts.iloc[0] >= semi_constant_threshold:\n",
        "                cols_to_drop.append(col)\n",
        "        except (TypeError, IndexError):\n",
        "            pass  # Handle cases where value_counts fails (e.g., mixed data types)\n",
        "\n",
        "    return df.drop(columns=cols_to_drop)\n",
        "\n",
        "def drop_non_numeric_columns(df):\n",
        "    \"\"\"Drops columns from a DataFrame that do not contain numeric values.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with non-numeric columns removed.\n",
        "    \"\"\"\n",
        "    numeric_df = df.select_dtypes(include=np.number)\n",
        "    return numeric_df\n",
        "def drop_high_correlation_columns(df, threshold=0.75):\n",
        "    \"\"\"Drops columns from a DataFrame that have a correlation above a specified threshold.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "        threshold: The correlation threshold above which columns are dropped.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with highly correlated columns removed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    corr_matrix = df.corr().abs()\n",
        "\n",
        "    # Select upper triangle of correlation matrix\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    # Find index of feature columns with correlation greater than 0.75\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "\n",
        "    # Drop features\n",
        "    return df.drop(columns=to_drop)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mjTpMu0nVGiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Landrum = True\n",
        "Phenol = False\n",
        "\n",
        "if Landrum:\n",
        "    if Phenol:\n",
        "        df_training = pd.read_csv(\"Landrum_Phenol_training_mordred.csv\")\n",
        "        df_test = pd.read_csv(\"Landrum_Phenol_test_mordred.csv\")\n",
        "    else:\n",
        "        df_training = pd.read_csv(\"Landrum_All_training_mordred.csv\")\n",
        "        df_test = pd.read_csv(\"Landrum_All_test_mordred.csv\")\n",
        "else:\n",
        "    if Phenol:\n",
        "        df_training = pd.read_csv(\"Minimal_Phenol_training_mordred.csv\")\n",
        "        df_test = pd.read_csv(\"Minimal_Phenol_test_mordred.csv\")\n",
        "    else:\n",
        "        df_training = pd.read_csv(\"Minimal_All_training_mordred.csv\")\n",
        "        df_test = pd.read_csv(\"Minimal_All_test_mordred.csv\")\n",
        "\n",
        "# === Normalize column names just once ===\n",
        "df_training.columns = df_training.columns.str.strip().str.lower()\n",
        "df_test.columns = df_test.columns.str.strip().str.lower()\n",
        "\n",
        "# Configuration switches\n",
        "USE_MORDRED = True       # True for Mordred, False for Morgan\n",
        "\n",
        "if USE_MORDRED:\n",
        "    # Drop non-numeric columns and NaNs\n",
        "    df_training = drop_non_numeric_columns(df_training)\n",
        "    df_training = df_training.dropna(axis=1)\n",
        "    df_test = drop_non_numeric_columns(df_test)\n",
        "    df_test = df_test.dropna(axis=1)\n",
        "\n",
        "    # Prepare features and labels\n",
        "    X_train = df_training.drop(columns=[\"active\", \"is_phenol\"], errors='ignore')\n",
        "    y_train = df_training[\"active\"]\n",
        "    X_test = df_test.drop(columns=[\"active\", \"is_phenol\"], errors='ignore')\n",
        "    y_test = df_test[\"active\"]\n",
        "\n",
        "\n",
        "else:\n",
        "    # Process Morgan fingerprints from SMILES\n",
        "    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
        "\n",
        "    def smiles_to_fp(smiles):\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return np.nan\n",
        "        return mfpgen.GetFingerprintAsNumPy(mol)\n",
        "\n",
        "    df_training[\"fp\"] = df_training[\"canonical_smiles\"].apply(smiles_to_fp)\n",
        "    df_training = df_training[df_training[\"fp\"].notna()].copy()\n",
        "    df_test[\"fp\"] = df_test[\"canonical_smiles\"].apply(smiles_to_fp)\n",
        "    df_test = df_test[df_test[\"fp\"].notna()].copy()\n",
        "\n",
        "\n",
        "    X_training = pd.DataFrame(df_training[\"fp\"].to_list())\n",
        "    y_training = df_training[\"active\"]\n",
        "    X_test = pd.DataFrame(df_test[\"fp\"].to_list())\n",
        "    y_test = df_test[\"active\"]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JcjiMHWoUFcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88961d68-c07e-4388-f4ca-0f08f2f8fe8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-bd99857b4b78>:9: DtypeWarning: Columns (7,8,9,10,11,12,13,14,15,16,17,18,56,57,58,59,60,61,62,63,64,143,144,152,153,155,156,157,158,159,160,161,170,171,179,180,188,189,197,198,206,207,215,216,224,225,233,234,236,237,238,239,240,241,242,243,244,263,264,265,266,267,268,269,270,271,344,345,346,347,348,349,350,359,360,368,369,371,372,373,374,375,376,377,386,387,395,396,404,405,413,414,422,423,431,432,440,441,449,450,452,453,454,455,456,457,465,466,473,474,476,477,478,479,480,481,489,490,497,498,505,506,513,514,521,522,529,530,537,538,545,546,548,549,550,551,552,553,561,562,569,570,572,573,574,575,576,577,585,586,593,594,601,602,609,610,617,618,625,626,633,634,641,642,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,783,784,820,828,834,835,836,844,850,851,882,883,884,885,886,887,888,889,890,891,892,893,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1366,1367,1369,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1585,1614) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_training = pd.read_csv(\"Landrum_All_training_mordred.csv\")\n",
            "<ipython-input-22-bd99857b4b78>:10: DtypeWarning: Columns (236,237,238,239,240,241,242,243,244,344,345,346,347,348,349,350,452,453,454,455,456,457,548,549,550,551,552,553,783,784,833,849,1585) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_test = pd.read_csv(\"Landrum_All_test_mordred.csv\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_np = X_train.to_numpy()\n",
        "y_train_np = y_train.to_numpy()\n",
        "X_test_np = X_test.to_numpy()\n",
        "y_test_np = y_test.to_numpy()\n",
        "\n",
        "print(X_train_np[:5])"
      ],
      "metadata": {
        "id": "sxkAf8bG5ASz",
        "outputId": "c95bc145-0b9d-4b15-ae04-aa35b71c838d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 13.23443095  11.54854293   1.         ...  82.          88.\n",
            "    4.11111111]\n",
            " [ 31.0649762   21.62099313   0.         ... 220.         272.\n",
            "    8.        ]\n",
            " [ 17.2868016   14.56386614   0.         ... 114.         131.\n",
            "    5.25      ]\n",
            " [ 23.84538077  19.03224991   0.         ... 158.         183.\n",
            "    6.83333333]\n",
            " [ 16.33240565  13.45331614   0.         ... 106.         120.\n",
            "    5.19444444]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model training"
      ],
      "metadata": {
        "id": "b-hx7MTkZicV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Preprocessing and Models\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def get_model_pipeline(model_name):\n",
        "    if model_name == 'RandomForest':\n",
        "        model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "    elif model_name == 'SVM':\n",
        "        model = SVC(class_weight='balanced', probability=True, random_state=42)\n",
        "    elif model_name == 'LogisticRegression':\n",
        "        model = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
        "    elif model_name == 'NeuralNetwork':\n",
        "        model = MLPClassifier(max_iter=1000, random_state=42)\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} is not supported.\")\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', MinMaxScaler()),\n",
        "        (\"var_thresh\", VarianceThreshold(threshold=0.01)),#light filtering of low variance features (more often seen than the 80% constant value method but should do approx the same)\n",
        "        (\"anova\", SelectKBest(score_func=mutual_info_classif)),  # Supervised Filter-Based Selection, k can be tuned (chi2 for morgan, f_classif for mordred, mutual_info_classif can handle both but takes long)\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "Kq4cDyFjZHxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grids = {\n",
        "    'RandomForest': {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [None, 10, 20],\n",
        "        'anova__k': [50, 100, 200],\n",
        "        'classifier__max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'SVM': {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'anova__k': [50, 100, 200],\n",
        "        'classifier__kernel': ['linear', 'rbf']\n",
        "    },\n",
        "    'LogisticRegression': {\n",
        "    'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'classifier__C': [0.1, 0.01, 0.001],\n",
        "    'classifier__solver': ['saga'],\n",
        "    'anova__k': [50, 100, 200],\n",
        "    'classifier__l1_ratio': [0.25, 0.5, 0.75]\n",
        "    },\n",
        "    'NeuralNetwork': {\n",
        "        'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'classifier__alpha': [0.0001, 0.001],\n",
        "        'anova__k': [50, 100, 200],\n",
        "        'classifier__learning_rate': ['constant', 'adaptive']\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "h-PUUt6FUxqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Tuning with Cross-Validation\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "def perform_grid_search(pipeline, param_grid, X_train, y_train):\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_grid=param_grid,\n",
        "        cv=kf,\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search\n"
      ],
      "metadata": {
        "id": "Y0A96ZSWVDCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, precision_score, f1_score, roc_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.base import clone\n",
        "\n",
        "#everything called test has nothing to do with test set (is validation but called test in scikit learn)\n",
        "def evaluate_model_with_cv(model, X, y, random_state):\n",
        "    \"\"\"\n",
        "    Evaluate a model using 10-fold cross-validation and return performance metrics.\n",
        "\n",
        "    Parameters:\n",
        "        model: The model to evaluate\n",
        "        X: Feature matrix\n",
        "        y: Target vector\n",
        "        random_state: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with performance metrics and arrays of per-fold metrics\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    # Initialize KFold\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
        "\n",
        "\n",
        "    # Lists to store metrics\n",
        "    metrics = {\n",
        "        'accuracies': [],\n",
        "        'precisions': [],\n",
        "        'f1_scores': [],\n",
        "        'auc_scores': [],\n",
        "        'kappa_scores': []\n",
        "    }\n",
        "\n",
        "    # Create figure for ROC curves\n",
        "    plt.figure()\n",
        "\n",
        "    # Iterate over folds\n",
        "    for k, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "        # Split data\n",
        "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "        y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Predict\n",
        "        y_pred_fold = model.predict(X_test_fold)\n",
        "        y_prob_fold = model.predict_proba(X_test_fold)[:, 1]\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics['accuracies'].append(accuracy_score(y_test_fold, y_pred_fold))\n",
        "        metrics['precisions'].append(precision_score(y_test_fold, y_pred_fold))\n",
        "        metrics['f1_scores'].append(f1_score(y_test_fold, y_pred_fold))\n",
        "        metrics['auc_scores'].append(roc_auc_score(y_test_fold, y_prob_fold))\n",
        "        metrics['kappa_scores'].append(cohen_kappa_score(y_test_fold, y_pred_fold))\n",
        "\n",
        "        # ROC curve\n",
        "        RocCurveDisplay.from_predictions(y_test_fold, y_prob_fold, name=f\"Fold {k+1}\", ax=plt.gca())\n",
        "\n",
        "\n",
        "    # Print metrics\n",
        "    metric_display_names = {\n",
        "    'accuracies': 'accuracy',\n",
        "    'precisions':'precision',\n",
        "    'f1_scores': 'F1 score',\n",
        "    'auc_scores': 'AUC score',\n",
        "    'kappa_scores': 'Cohens kappa'\n",
        "    }\n",
        "\n",
        "    for metric_name, values in metrics.items():\n",
        "      display_name = metric_display_names[metric_name]\n",
        "      print(f\"Average {display_name}: {np.mean(values):.4f} \\t and std err: {stats.sem(values):.4f}\")\n",
        "    print(f\"Time taken : {time.time() - t0:.2f}s\\n\")\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "X6rxmxMHNI8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through each model, perform grid search, and evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import sem\n",
        "for model_name in ['RandomForest']: #add models to train: 'NeuralNetwork','RandomForest', 'SVM', 'LogisticRegression',\n",
        "    print(f\"\\nTraining and evaluating {model_name}...\")\n",
        "    t0 = time.time()\n",
        "    pipeline = get_model_pipeline(model_name)\n",
        "    param_grid = param_grids[model_name]\n",
        "    grid_search = perform_grid_search(pipeline, param_grid, X_train, y_train)\n",
        "    model = grid_search.best_estimator_\n",
        "    print(f\"Time taken : {time.time() - t0:.2f}s\\n\")\n",
        "    print(f\"Hyperparameters: {grid_search.best_params_}\\n\")\n",
        "    evaluate_model_with_cv(model, X_train, y_train, random_state=22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8ew0D9LjsYz",
        "outputId": "d1201162-bdf2-4f25-c7ce-8c22ece7af04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and evaluating NeuralNetwork...\n"
          ]
        }
      ]
    }
  ]
}